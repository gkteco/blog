[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gianni Crivello’s Blog",
    "section": "",
    "text": "Hi! My name is Gianni Crivello and I’m a Solutions Engineer specializing in AI / ML. I love solving problems and building things. I’m mostly self taught with a hackers background in AI / ML, but I’m a bit of a generalist - I enjoy leanring about compilers, library/language design, algorithms, operating systems, and other low-level system stuff.\nI currently work at Techolution helping our customers solve hard problems and build great products.",
    "crumbs": [
      "Gianni Crivello's Blog"
    ]
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Gianni Crivello’s Blog",
    "section": "",
    "text": "Hi! My name is Gianni Crivello and I’m a Solutions Engineer specializing in AI / ML. I love solving problems and building things. I’m mostly self taught with a hackers background in AI / ML, but I’m a bit of a generalist - I enjoy leanring about compilers, library/language design, algorithms, operating systems, and other low-level system stuff.\nI currently work at Techolution helping our customers solve hard problems and build great products.",
    "crumbs": [
      "Gianni Crivello's Blog"
    ]
  },
  {
    "objectID": "datablock walkthrough.html",
    "href": "datablock walkthrough.html",
    "title": "DataBlock Walkthrough",
    "section": "",
    "text": "The DataBlock API from fastai is a powerful library for building custom data processing pipelines. It’s a core part of the fastai high level api workflow.\n\n\n\nimage.png\n\n\nQuick note on fastai’s API Design\nFastai’s API design philosophy emphasizes simplicity and flexibility. This is an approach that I really appreciate. When I evaluate a new library, It’s natural for me to build something simple to get a quick win, and iterativly build up to a more complex application. In my line of work, being able to scale down and prove out a concept is just as important as being able to scale up.\nKey aspects of fastai’s API design that I think are important are:\n\nSensible defaults: Many functions come with pre-configured settings that work well for common use cases. You’ll see this in action with the DataBlock API. Having your ML framework choose sensible defaults based on a type system is something I really like.\nLayered API: The library provides both high-level APIs for rapid development and low-level APIs for fine-grained control. This goes back to the idea of being able to prove a concept quickly without having to rewrite everything once you’re ready to scale. Fastai is built on top of PyTorch, so the deeper you go into the library, the more you’ll just be using PyTorch. Which is also great!\nThe library is very well documented, very hackable, and is easy to understand. This is a huge benefit for those of us that don’t have a large team and need to really be able to rely on the library’s documentation for more complex applications.\n\nGreat, now let’s go DataBlock crazy!\nStart by importing the modules we’ll need for this walkthrough.\n\n# we are now going to download the pets dataset from URLs this returns a pathlib object\npath = untar_data(URLs.PETS)\npath\n\nPath('/Users/giannicrivello/.fastai/data/oxford-iiit-pet')\n\n\nIf you’re ever curious about what you are working with in fastai, assuming you’re in a notebook, you can user the ?? command to get a quick overview of the object’s source code. You can also use show_doc to get a detailed description of the object’s API.\n\nsource\n\n\n\n untar_data (url:str, archive:pathlib.Path=None, data:pathlib.Path=None,\n             c_key:str='data', force_download:bool=False,\n             base:str='~/.fastai')\n\nDownload url using FastDownload.get\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\nstr\n\nFile to download\n\n\narchive\nPath\nNone\nOptional override for Config’s archive key\n\n\ndata\nPath\nNone\nOptional override for Config’s data key\n\n\nc_key\nstr\ndata\nKey in Config where to extract file\n\n\nforce_download\nbool\nFalse\nSetting to True will overwrite any existing copy of data\n\n\nbase\nstr\n~/.fastai\nDirectory containing config file and base of relative paths\n\n\nReturns\nPath\n\nPath to extracted file(s)\n\n\n\n\n# here is the source code for `URLs`\nURLs??\n\nInit signature: URLs()\nSource:        \nclass URLs():\n    \"Global constants for dataset and model URLs.\"\n    LOCAL_PATH = Path.cwd()\n    MDL = 'http://files.fast.ai/models/'\n    GOOGLE = 'https://storage.googleapis.com/'\n    S3  = 'https://s3.amazonaws.com/fast-ai-'\n    URL = f'{S3}sample/'\n\n    S3_IMAGE    = f'{S3}imageclas/'\n    S3_IMAGELOC = f'{S3}imagelocal/'\n    S3_AUDI     = f'{S3}audio/'\n    S3_NLP      = f'{S3}nlp/'\n    S3_COCO     = f'{S3}coco/'\n    S3_MODEL    = f'{S3}modelzoo/'\n\n    # main datasets\n    ADULT_SAMPLE        = f'{URL}adult_sample.tgz'\n    BIWI_SAMPLE         = f'{URL}biwi_sample.tgz'\n    CIFAR               = f'{URL}cifar10.tgz'\n    COCO_SAMPLE         = f'{S3_COCO}coco_sample.tgz'\n    COCO_TINY           = f'{S3_COCO}coco_tiny.tgz'\n    HUMAN_NUMBERS       = f'{URL}human_numbers.tgz'\n    IMDB                = f'{S3_NLP}imdb.tgz'\n    IMDB_SAMPLE         = f'{URL}imdb_sample.tgz'\n    ML_SAMPLE           = f'{URL}movie_lens_sample.tgz'\n    ML_100k             = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n    MNIST_SAMPLE        = f'{URL}mnist_sample.tgz'\n    MNIST_TINY          = f'{URL}mnist_tiny.tgz'\n    MNIST_VAR_SIZE_TINY = f'{S3_IMAGE}mnist_var_size_tiny.tgz'\n    PLANET_SAMPLE       = f'{URL}planet_sample.tgz'\n    PLANET_TINY         = f'{URL}planet_tiny.tgz'\n    IMAGENETTE          = f'{S3_IMAGE}imagenette2.tgz'\n    IMAGENETTE_160      = f'{S3_IMAGE}imagenette2-160.tgz'\n    IMAGENETTE_320      = f'{S3_IMAGE}imagenette2-320.tgz'\n    IMAGEWOOF           = f'{S3_IMAGE}imagewoof2.tgz'\n    IMAGEWOOF_160       = f'{S3_IMAGE}imagewoof2-160.tgz'\n    IMAGEWOOF_320       = f'{S3_IMAGE}imagewoof2-320.tgz'\n    IMAGEWANG           = f'{S3_IMAGE}imagewang.tgz'\n    IMAGEWANG_160       = f'{S3_IMAGE}imagewang-160.tgz'\n    IMAGEWANG_320       = f'{S3_IMAGE}imagewang-320.tgz'\n\n    # kaggle competitions download dogs-vs-cats -p {DOGS.absolute()}\n    DOGS = f'{URL}dogscats.tgz'\n\n    # image classification datasets\n    CALTECH_101  = f'{S3_IMAGE}caltech_101.tgz'\n    CARS         = f'{S3_IMAGE}stanford-cars.tgz'\n    CIFAR_100    = f'{S3_IMAGE}cifar100.tgz'\n    CUB_200_2011 = f'{S3_IMAGE}CUB_200_2011.tgz'\n    FLOWERS      = f'{S3_IMAGE}oxford-102-flowers.tgz'\n    FOOD         = f'{S3_IMAGE}food-101.tgz'\n    MNIST        = f'{S3_IMAGE}mnist_png.tgz'\n    PETS         = f'{S3_IMAGE}oxford-iiit-pet.tgz'\n\n    # NLP datasets\n    AG_NEWS                 = f'{S3_NLP}ag_news_csv.tgz'\n    AMAZON_REVIEWS          = f'{S3_NLP}amazon_review_full_csv.tgz'\n    AMAZON_REVIEWS_POLARITY = f'{S3_NLP}amazon_review_polarity_csv.tgz'\n    DBPEDIA                 = f'{S3_NLP}dbpedia_csv.tgz'\n    MT_ENG_FRA              = f'{S3_NLP}giga-fren.tgz'\n    SOGOU_NEWS              = f'{S3_NLP}sogou_news_csv.tgz'\n    WIKITEXT                = f'{S3_NLP}wikitext-103.tgz'\n    WIKITEXT_TINY           = f'{S3_NLP}wikitext-2.tgz'\n    YAHOO_ANSWERS           = f'{S3_NLP}yahoo_answers_csv.tgz'\n    YELP_REVIEWS            = f'{S3_NLP}yelp_review_full_csv.tgz'\n    YELP_REVIEWS_POLARITY   = f'{S3_NLP}yelp_review_polarity_csv.tgz'\n\n    # Image localization datasets\n    BIWI_HEAD_POSE     = f\"{S3_IMAGELOC}biwi_head_pose.tgz\"\n    CAMVID             = f'{S3_IMAGELOC}camvid.tgz'\n    CAMVID_TINY        = f'{URL}camvid_tiny.tgz'\n    LSUN_BEDROOMS      = f'{S3_IMAGE}bedroom.tgz'\n    PASCAL_2007        = f'{S3_IMAGELOC}pascal_2007.tgz'\n    PASCAL_2012        = f'{S3_IMAGELOC}pascal_2012.tgz'\n\n    # Audio classification datasets\n    MACAQUES           = f'{GOOGLE}ml-animal-sounds-datasets/macaques.zip'\n    ZEBRA_FINCH        = f'{GOOGLE}ml-animal-sounds-datasets/zebra_finch.zip'\n\n    # Medical Imaging datasets\n    #SKIN_LESION        = f'{S3_IMAGELOC}skin_lesion.tgz'\n    SIIM_SMALL         = f'{S3_IMAGELOC}siim_small.tgz'\n    TCGA_SMALL         = f'{S3_IMAGELOC}tcga_small.tgz'\n\n    #Pretrained models\n    OPENAI_TRANSFORMER = f'{S3_MODEL}transformer.tgz'\n    WT103_FWD          = f'{S3_MODEL}wt103-fwd.tgz'\n    WT103_BWD          = f'{S3_MODEL}wt103-bwd.tgz'\n\n    def path(\n        url:str='.', # File to download\n        c_key:str='archive' # Key in `Config` where to save URL\n    ) -&gt; Path:\n        \"Local path where to download based on `c_key`\"\n        fname = url.split('/')[-1]\n        local_path = URLs.LOCAL_PATH/('models' if c_key=='model' else 'data')/fname\n        if local_path.exists(): return local_path\n        return fastai_path(c_key)/fname\nFile:           ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/external.py\nType:           type\nSubclasses:     \n\n\n\n# let's instantiate a `DataBlock` object\ndblock = DataBlock()\n\n\n# by itself, a `DataBlock` is just a blueprint for how to assemple your data. For us to do \n# anything, useful with it we need to pass in a source and what we want the dblock to return.\n\n# You get two options - A `Datasets` object or a `DataLoaders` object.\n\ndsets = dblock.datasets(get_image_files(path/\"images\"))\n\n\n# We will see our file repeated twice, that is because by default,\n# the `DataBlock` API assumes we have an input and a target and will wrape it in a tuple.\ndsets.train[0]\n\n(Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_152.jpg'),\n Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_152.jpg'))\n\n\nIf we look at the type…\n\ntype(dsets)\n\nfastai.data.core.Datasets\n\n\nWe just stamped out a Datasets object. Let’s dive a little deeper into what we just created.\n\n\n\nDatasets can be thought of as a object that returns a tuple from items, such as (input, target) and applies a Pipeline or Transform to each item in items.\nSometing important to note is that for each transform in tfms, it creates an element in the tuple. So that means if you have two transforms, you’ll have a tuple of length 2, if you have three transforms, you’ll have a tuple of length 3, and so on. Let’s see an exmample.\n\n# items we are going to pass into `Datasets`\nitems = [1, 2, 3, 4]\n\n# making a datasets object with no transforms will product a tuple of length 1\n# basicially, just the item in items wrapped in a tuple \n# [1, 2, 3, 4] -&gt; [(1, ), (2, ), (3, ), (4, )]\n\ndsets = Datasets(items)\ndsets\n\n(#4) [(1,),(2,),(3,),(4,)]\n\n\n\n# Now let's see what happens when we add a list of transforms\n# in this case, I'm giving the argument `tfms` a list of two identity functions\n# we'll get back out a tuple of length 2 packing up our list into a list of tuples\n# [1, 2, 3, 4] -&gt; [(1, 1), (2, 2), (3, 3), (4, 4)]\n\ndsets = Datasets(items, tfms=[lambda i: i, lambda i: i])\ndsets\n\n(#4) [(1, 1),(2, 2),(3, 3),(4, 4)]\n\n\n\n# Let's get more creative and create a dataset where our dependent variable is the item\n# in items and our dependent variable is a the result of the function f(x) = x**2\n# [1, 2, 3, 4] -&gt; [(1, 1), (2, 4), (3, 9), (4, 16)]\ndsets = Datasets(items, tfms=[lambda i: i, lambda i: i**2])\ndsets\n\n(#4) [(1, 1),(2, 4),(3, 9),(4, 16)]\n\n\n\n# or let's build a dataset where we output a tuple of length 3\ndsets = Datasets(items, tfms=[lambda i: i, lambda i: i**2, lambda i: i**3])\ndsets\n\n(#4) [(1, 1, 1),(2, 4, 8),(3, 9, 27),(4, 16, 64)]\n\n\n\n# Let's get even more creative! Let's say I had a list of transforms for the dependent variable\n# and a list of transforms for the independent variable. Can we do this? Yes we can!\n\ndsets = Datasets(items, tfms=[[lambda i: i**2, lambda i: i+1], [lambda i: i**2, lambda i: i+2]])\ndsets\n\n(#4) [(2, 3),(5, 6),(10, 11),(17, 18)]\n\n\n\n# We can also use a `Pipeline` object to chain our transforms together.\npipe = Pipeline([lambda i: i**2, lambda i: i+1])\n\n# now we have a pipeline that will take x and return x**2 + 1\ntest_eq(pipe(2), 5)\ntest_eq(pipe(3), 10)\n\n\n# We can use this pipeline to create a dataset.\ndsets = Datasets(items, tfms=[pipe, pipe])\ndsets\n# We can also use a `Transform` object to transform our data.\n\n(#4) [(2, 2),(5, 5),(10, 10),(17, 17)]\n\n\nWhat if we want to split our data into a training and validation set?\nThis is where we can tell Datasets to split our data into a training and validation set. I personally like to use python’s builtin slice function to do this. Slices in python are great, it’s a flexible way for us to tell Datasets which indices we want to start and stop for both our training and validation set.\n\nimport numpy as np\n\n# Generate 1000 data points\ndata = np.array([np.random.randint(0, 10, dtype=int) for _ in range(1000)]).reshape(1, -1)[0]\n\n# find the index of where we plan to split out data\nsplit = int(len(data) * 0.8)\n\n# pass in our data, our transforms, and our split in the form of a list of slices\ndset = Datasets(data, \n                tfms=[lambda x: x**2, lambda x: x+1],\n                splits=[slice(0, split), slice(split, len(data))]\n                )\n# Let's inspect our data now\nprint(dset.train[0])\nprint(dset.valid[0])\nprint(f\" Lenght of train: {len(dset.train)}, Length of valid: {len(dset.valid)}\")\n\n(81, 10)\n(16, 5)\n Lenght of train: 800, Length of valid: 200\n\n\nThis wraps up my digression into Datasets. Knowing how they work can help us understand what DataBlock is doing for you when you call DataBlock.datasets. Knowing what Datasets is expecting is important (atleast for me) when i’m in data munging madness and need to debug complex data pipelines.\nLet’s get back to DataBlock.\n\n# Remember, `DataBlock` is just a blueprint on how to construct a `Datasets` or \n# `DataLoaders` object. We can give `DataBlock` more information on how to stamp out\n# our data objects.\n\ndblock = DataBlock(get_items = get_image_files)\n\n# Now we can pass a source to our call to `datasets` and we can get the same result as before\n\ndsets = dblock.datasets(path/\"images\")\ndsets[0]\n\n(Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'),\n Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'))\n\n\n\n# Now that we know more information about `Datasets`, we can make our `DataBlock` more useful\n# our tuples (input, target) contain a repeating file path. Let's clean this up a little.\n\ndsets = DataBlock(\n    get_items = get_image_files,\n    get_y = lambda x: x.name,\n).datasets(path/\"images\")\n\n# Now our tuple should be the result of (filepath, label)\ndsets[0]\n\n(Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'),\n 'Egyptian_Mau_167.jpg')\n\n\nWe see that our label is the label of the file. For this we passed in a function to get_y. In the case of what we are going to use as an end-to-end example of using a DataBlock to train a model, we will need to create a function that will assign a label depending on a class that will be in our vocab.\nSo, for our example, let’s make a simple classification model, let’s predict whether or not a given image is a dog.\nFor this, our label will just be “cat” or “dog”.\n\n# our get y function\ndef get_y(x):\n    return \"cat\" if x.name[0].isupper() else \"dog\"\n\ndblock = DataBlock(\n    get_items = get_image_files,\n    get_y     = get_y,\n)\n\ndsets = dblock.datasets(path/\"images\")\ndsets.train[0]\n\n(Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/basset_hound_199.jpg'),\n 'dog')\n\n\nNice. Now comes the interesting part of the DataBlock API and that is it’s type system.\nI can tell DataBlock what type of data I’m working with and it will give me a good amount of reasonable defaults, all the way down to the loss function I should use. This is really cool for me, going back to what I said earlier about “scaling down”. Most of my work requires me to be fast, really fast. Having a solid foundation of what type of data I’m working with, then having fastai do a lot of the tedious work under the hood for me is something I find really cool.\nYes, sometimes I want to dig deep into a new loss function, or optimizer…..but most times, I just want something to work. Fastai is the best DSL i’ve come into use with.\nLet’s see what I’m talking about.\n\n# I know that I want to predicit a label (cat or dog) from an image.\n# So, I can tell `DataBlock` more information about my data by specifying blocks\ndblock = DataBlock(\n    blocks = (ImageBlock, CategoryBlock), # this is what is new!\n    get_items = get_image_files,\n    get_y = get_y,\n)\n\n# by specifying blocks, we are telling `DataBlock` that we are working with a tuple of \n# (image, category). If we now stamp out a `Datasets` object, you'll see extra magic happen.\n\ndsets = dblock.datasets(path/\"images\")\ndsets[0]\n\n(PILImage mode=RGB size=183x275, TensorCategory(0))\n\n\n\n# Now let's apply transformations to resize our images to a consistent size\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=get_y,\n    item_tfms=RandomResizedCrop(224, 224),\n)\n\n# Create datasets with the new DataBlock\nds = dblock.datasets(path/\"images\")\nds[0]\n\n(PILImage mode=RGB size=183x275, TensorCategory(0))\n\n\nThat is interesting. Let’s inspect the data further by opening and displaying the image and looking up our label in the vocab.\n\n# Let's open and display the image\nimg, label = dsets[0]\nimg.show()\n\n# Print the label\nprint(f\"Label: {dsets.vocab[label]}\")\n\nLabel: cat\n\n\n\n\n\n\n\n\n\n\n# Let's now tell `DataBlock` how to split our data for training and validation.\n\ndblock = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = get_y,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42), # split 20% of data to validation\n)\n\n# let's inspect our dblock with `summary`\ndblock.summary(path/\"images\")\n\nSetting-up type transforms pipelines\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=375x500\n  Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying get_y gives\n      dog\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(1)\n\nFinal sample: (PILImage mode=RGB size=375x500, TensorCategory(1))\n\n\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImage mode=RGB size=375x500, TensorCategory(1))\n    applying ToTensor gives\n      (TensorImage of size 3x500x375, TensorCategory(1))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\nError! It's not possible to collate your items in a batch\nCould not collate the 0-th members of your tuples because got the following shapes\ntorch.Size([3, 500, 375]),torch.Size([3, 333, 500]),torch.Size([3, 352, 500]),torch.Size([3, 500, 354])\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[154], line 11\n      3 dblock = DataBlock(\n      4     blocks = (ImageBlock, CategoryBlock),\n      5     get_items = get_image_files,\n      6     get_y = get_y,\n      7     splitter = RandomSplitter(valid_pct=0.2, seed=42), # split 20% of data to validation\n      8 )\n     10 # let's inspect our dblock with `summary`\n---&gt; 11 dblock.summary(path/\"images\")\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/block.py:237, in summary(self, source, bs, show_batch, **kwargs)\n    235     why = _find_fail_collate(s)\n    236     print(\"Make sure all parts of your samples are tensors of the same size\" if why is None else why)\n--&gt; 237     raise e\n    239 if len([f for f in dls.train.after_batch.fs if f.name != 'noop'])!=0:\n    240     print(\"\\nApplying batch_tfms to the batch built\")\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/block.py:231, in summary(self, source, bs, show_batch, **kwargs)\n    229 print(\"\\nCollating items in a batch\")\n    230 try:\n--&gt; 231     b = dls.train.create_batch(s)\n    232     b = retain_types(b, s[0] if is_listy(s) else s)\n    233 except Exception as e:\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:181, in DataLoader.create_batch(self, b)\n    179 try: return (fa_collate,fa_convert)[self.prebatched](b)\n    180 except Exception as e: \n--&gt; 181     if not self.prebatched: collate_error(e,b)\n    182     raise\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:179, in DataLoader.create_batch(self, b)\n    178 def create_batch(self, b): \n--&gt; 179     try: return (fa_collate,fa_convert)[self.prebatched](b)\n    180     except Exception as e: \n    181         if not self.prebatched: collate_error(e,b)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:52, in fa_collate(t)\n     49 \"A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\"\n     50 b = t[0]\n     51 return (default_collate(t) if isinstance(b, _collate_types)\n---&gt; 52         else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     53         else default_collate(t))\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:52, in &lt;listcomp&gt;(.0)\n     49 \"A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\"\n     50 b = t[0]\n     51 return (default_collate(t) if isinstance(b, _collate_types)\n---&gt; 52         else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     53         else default_collate(t))\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:51, in fa_collate(t)\n     49 \"A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\"\n     50 b = t[0]\n---&gt; 51 return (default_collate(t) if isinstance(b, _collate_types)\n     52         else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     53         else default_collate(t))\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:317, in default_collate(batch)\n    256 def default_collate(batch):\n    257     r\"\"\"\n    258     Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\n    259 \n   (...)\n    315         &gt;&gt;&gt; default_collate(batch)  # Handle `CustomType` automatically\n    316     \"\"\"\n--&gt; 317     return collate(batch, collate_fn_map=default_collate_fn_map)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:146, in collate(batch, collate_fn_map)\n    144     for collate_type in collate_fn_map:\n    145         if isinstance(elem, collate_type):\n--&gt; 146             return collate_fn_map[collate_type](batch, collate_fn_map=collate_fn_map)\n    148 if isinstance(elem, collections.abc.Mapping):\n    149     try:\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:214, in collate_tensor_fn(batch, collate_fn_map)\n    212     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n    213     out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n--&gt; 214 return torch.stack(batch, 0, out=out)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/torch_core.py:382, in TensorBase.__torch_function__(cls, func, types, args, kwargs)\n    380 if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n    381 if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n--&gt; 382 res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n    383 dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n    384 if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/_tensor.py:1437, in Tensor.__torch_function__(cls, func, types, args, kwargs)\n   1434     return NotImplemented\n   1436 with _C.DisableTorchFunctionSubclass():\n-&gt; 1437     ret = func(*args, **kwargs)\n   1438     if func in get_default_nowrap_functions():\n   1439         return ret\n\nRuntimeError: Error when trying to collate the data into batches with fa_collate, at least two tensors in the batch are not the same size.\n\nMismatch found on axis 0 of the batch and is of type `TensorImage`:\n    Item at index 0 has shape: torch.Size([3, 500, 375])\n    Item at index 1 has shape: torch.Size([3, 333, 500])\n\nPlease include a transform in `after_item` that ensures all data of type TensorImage is the same size\n\n\n\nOpps! We have an error. If we follow the trace that summary gives us, we can see that we are able to build up a single example of our data easily (which means we can bulid up a Datasets object). However, we break when we attempt to collate our data into batches.\nWhy is that? Well, our summary tels us:\nMismatch found on axis 0 of the batch and is of type `TensorImage`:\n    Item at index 0 has shape: torch.Size([3, 500, 375])\n    Item at index 1 has shape: torch.Size([3, 333, 500])\nOur images are of different sizes! DataBlock gives us a method to change this. We can pass in a transform to item_tfms to apply a transform to our independent variables. This should fix the issue.\n\ndblock = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = get_y,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    item_tfms = Resize(224), # resize each image to 224x224\n)\n\ndblock.summary(path/\"images\")\n\nSetting-up type transforms pipelines\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=375x500\n  Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying get_y gives\n      dog\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(1)\n\nFinal sample: (PILImage mode=RGB size=375x500, TensorCategory(1))\n\n\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=375x500, TensorCategory(1))\n    applying Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} gives\n      (PILImage mode=RGB size=224x224, TensorCategory(1))\n    applying ToTensor gives\n      (TensorImage of size 3x224x224, TensorCategory(1))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImage of size 4x3x224x224, TensorCategory([1, 0, 1, 1], device='mps:0'))\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([1, 0, 1, 1], device='mps:0'))\n\n\nAwesome. Now we are ready to stamp out our Dataloaders object and train a model. But first, let’s take a quick digresstion to talk about Dataloaders.",
    "crumbs": [
      "DataBlock Walkthrough"
    ]
  },
  {
    "objectID": "datablock walkthrough.html#datasets",
    "href": "datablock walkthrough.html#datasets",
    "title": "DataBlock Walkthrough",
    "section": "",
    "text": "Datasets can be thought of as a object that returns a tuple from items, such as (input, target) and applies a Pipeline or Transform to each item in items.\nSometing important to note is that for each transform in tfms, it creates an element in the tuple. So that means if you have two transforms, you’ll have a tuple of length 2, if you have three transforms, you’ll have a tuple of length 3, and so on. Let’s see an exmample.\n\n# items we are going to pass into `Datasets`\nitems = [1, 2, 3, 4]\n\n# making a datasets object with no transforms will product a tuple of length 1\n# basicially, just the item in items wrapped in a tuple \n# [1, 2, 3, 4] -&gt; [(1, ), (2, ), (3, ), (4, )]\n\ndsets = Datasets(items)\ndsets\n\n(#4) [(1,),(2,),(3,),(4,)]\n\n\n\n# Now let's see what happens when we add a list of transforms\n# in this case, I'm giving the argument `tfms` a list of two identity functions\n# we'll get back out a tuple of length 2 packing up our list into a list of tuples\n# [1, 2, 3, 4] -&gt; [(1, 1), (2, 2), (3, 3), (4, 4)]\n\ndsets = Datasets(items, tfms=[lambda i: i, lambda i: i])\ndsets\n\n(#4) [(1, 1),(2, 2),(3, 3),(4, 4)]\n\n\n\n# Let's get more creative and create a dataset where our dependent variable is the item\n# in items and our dependent variable is a the result of the function f(x) = x**2\n# [1, 2, 3, 4] -&gt; [(1, 1), (2, 4), (3, 9), (4, 16)]\ndsets = Datasets(items, tfms=[lambda i: i, lambda i: i**2])\ndsets\n\n(#4) [(1, 1),(2, 4),(3, 9),(4, 16)]\n\n\n\n# or let's build a dataset where we output a tuple of length 3\ndsets = Datasets(items, tfms=[lambda i: i, lambda i: i**2, lambda i: i**3])\ndsets\n\n(#4) [(1, 1, 1),(2, 4, 8),(3, 9, 27),(4, 16, 64)]\n\n\n\n# Let's get even more creative! Let's say I had a list of transforms for the dependent variable\n# and a list of transforms for the independent variable. Can we do this? Yes we can!\n\ndsets = Datasets(items, tfms=[[lambda i: i**2, lambda i: i+1], [lambda i: i**2, lambda i: i+2]])\ndsets\n\n(#4) [(2, 3),(5, 6),(10, 11),(17, 18)]\n\n\n\n# We can also use a `Pipeline` object to chain our transforms together.\npipe = Pipeline([lambda i: i**2, lambda i: i+1])\n\n# now we have a pipeline that will take x and return x**2 + 1\ntest_eq(pipe(2), 5)\ntest_eq(pipe(3), 10)\n\n\n# We can use this pipeline to create a dataset.\ndsets = Datasets(items, tfms=[pipe, pipe])\ndsets\n# We can also use a `Transform` object to transform our data.\n\n(#4) [(2, 2),(5, 5),(10, 10),(17, 17)]\n\n\nWhat if we want to split our data into a training and validation set?\nThis is where we can tell Datasets to split our data into a training and validation set. I personally like to use python’s builtin slice function to do this. Slices in python are great, it’s a flexible way for us to tell Datasets which indices we want to start and stop for both our training and validation set.\n\nimport numpy as np\n\n# Generate 1000 data points\ndata = np.array([np.random.randint(0, 10, dtype=int) for _ in range(1000)]).reshape(1, -1)[0]\n\n# find the index of where we plan to split out data\nsplit = int(len(data) * 0.8)\n\n# pass in our data, our transforms, and our split in the form of a list of slices\ndset = Datasets(data, \n                tfms=[lambda x: x**2, lambda x: x+1],\n                splits=[slice(0, split), slice(split, len(data))]\n                )\n# Let's inspect our data now\nprint(dset.train[0])\nprint(dset.valid[0])\nprint(f\" Lenght of train: {len(dset.train)}, Length of valid: {len(dset.valid)}\")\n\n(81, 10)\n(16, 5)\n Lenght of train: 800, Length of valid: 200\n\n\nThis wraps up my digression into Datasets. Knowing how they work can help us understand what DataBlock is doing for you when you call DataBlock.datasets. Knowing what Datasets is expecting is important (atleast for me) when i’m in data munging madness and need to debug complex data pipelines.\nLet’s get back to DataBlock.\n\n# Remember, `DataBlock` is just a blueprint on how to construct a `Datasets` or \n# `DataLoaders` object. We can give `DataBlock` more information on how to stamp out\n# our data objects.\n\ndblock = DataBlock(get_items = get_image_files)\n\n# Now we can pass a source to our call to `datasets` and we can get the same result as before\n\ndsets = dblock.datasets(path/\"images\")\ndsets[0]\n\n(Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'),\n Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'))\n\n\n\n# Now that we know more information about `Datasets`, we can make our `DataBlock` more useful\n# our tuples (input, target) contain a repeating file path. Let's clean this up a little.\n\ndsets = DataBlock(\n    get_items = get_image_files,\n    get_y = lambda x: x.name,\n).datasets(path/\"images\")\n\n# Now our tuple should be the result of (filepath, label)\ndsets[0]\n\n(Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/Egyptian_Mau_167.jpg'),\n 'Egyptian_Mau_167.jpg')\n\n\nWe see that our label is the label of the file. For this we passed in a function to get_y. In the case of what we are going to use as an end-to-end example of using a DataBlock to train a model, we will need to create a function that will assign a label depending on a class that will be in our vocab.\nSo, for our example, let’s make a simple classification model, let’s predict whether or not a given image is a dog.\nFor this, our label will just be “cat” or “dog”.\n\n# our get y function\ndef get_y(x):\n    return \"cat\" if x.name[0].isupper() else \"dog\"\n\ndblock = DataBlock(\n    get_items = get_image_files,\n    get_y     = get_y,\n)\n\ndsets = dblock.datasets(path/\"images\")\ndsets.train[0]\n\n(Path('/Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/basset_hound_199.jpg'),\n 'dog')\n\n\nNice. Now comes the interesting part of the DataBlock API and that is it’s type system.\nI can tell DataBlock what type of data I’m working with and it will give me a good amount of reasonable defaults, all the way down to the loss function I should use. This is really cool for me, going back to what I said earlier about “scaling down”. Most of my work requires me to be fast, really fast. Having a solid foundation of what type of data I’m working with, then having fastai do a lot of the tedious work under the hood for me is something I find really cool.\nYes, sometimes I want to dig deep into a new loss function, or optimizer…..but most times, I just want something to work. Fastai is the best DSL i’ve come into use with.\nLet’s see what I’m talking about.\n\n# I know that I want to predicit a label (cat or dog) from an image.\n# So, I can tell `DataBlock` more information about my data by specifying blocks\ndblock = DataBlock(\n    blocks = (ImageBlock, CategoryBlock), # this is what is new!\n    get_items = get_image_files,\n    get_y = get_y,\n)\n\n# by specifying blocks, we are telling `DataBlock` that we are working with a tuple of \n# (image, category). If we now stamp out a `Datasets` object, you'll see extra magic happen.\n\ndsets = dblock.datasets(path/\"images\")\ndsets[0]\n\n(PILImage mode=RGB size=183x275, TensorCategory(0))\n\n\n\n# Now let's apply transformations to resize our images to a consistent size\ndblock = DataBlock(\n    blocks=(ImageBlock, CategoryBlock),\n    get_items=get_image_files,\n    get_y=get_y,\n    item_tfms=RandomResizedCrop(224, 224),\n)\n\n# Create datasets with the new DataBlock\nds = dblock.datasets(path/\"images\")\nds[0]\n\n(PILImage mode=RGB size=183x275, TensorCategory(0))\n\n\nThat is interesting. Let’s inspect the data further by opening and displaying the image and looking up our label in the vocab.\n\n# Let's open and display the image\nimg, label = dsets[0]\nimg.show()\n\n# Print the label\nprint(f\"Label: {dsets.vocab[label]}\")\n\nLabel: cat\n\n\n\n\n\n\n\n\n\n\n# Let's now tell `DataBlock` how to split our data for training and validation.\n\ndblock = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = get_y,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42), # split 20% of data to validation\n)\n\n# let's inspect our dblock with `summary`\ndblock.summary(path/\"images\")\n\nSetting-up type transforms pipelines\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=375x500\n  Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying get_y gives\n      dog\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(1)\n\nFinal sample: (PILImage mode=RGB size=375x500, TensorCategory(1))\n\n\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ToTensor\n    starting from\n      (PILImage mode=RGB size=375x500, TensorCategory(1))\n    applying ToTensor gives\n      (TensorImage of size 3x500x375, TensorCategory(1))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\nError! It's not possible to collate your items in a batch\nCould not collate the 0-th members of your tuples because got the following shapes\ntorch.Size([3, 500, 375]),torch.Size([3, 333, 500]),torch.Size([3, 352, 500]),torch.Size([3, 500, 354])\n\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[154], line 11\n      3 dblock = DataBlock(\n      4     blocks = (ImageBlock, CategoryBlock),\n      5     get_items = get_image_files,\n      6     get_y = get_y,\n      7     splitter = RandomSplitter(valid_pct=0.2, seed=42), # split 20% of data to validation\n      8 )\n     10 # let's inspect our dblock with `summary`\n---&gt; 11 dblock.summary(path/\"images\")\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/block.py:237, in summary(self, source, bs, show_batch, **kwargs)\n    235     why = _find_fail_collate(s)\n    236     print(\"Make sure all parts of your samples are tensors of the same size\" if why is None else why)\n--&gt; 237     raise e\n    239 if len([f for f in dls.train.after_batch.fs if f.name != 'noop'])!=0:\n    240     print(\"\\nApplying batch_tfms to the batch built\")\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/block.py:231, in summary(self, source, bs, show_batch, **kwargs)\n    229 print(\"\\nCollating items in a batch\")\n    230 try:\n--&gt; 231     b = dls.train.create_batch(s)\n    232     b = retain_types(b, s[0] if is_listy(s) else s)\n    233 except Exception as e:\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:181, in DataLoader.create_batch(self, b)\n    179 try: return (fa_collate,fa_convert)[self.prebatched](b)\n    180 except Exception as e: \n--&gt; 181     if not self.prebatched: collate_error(e,b)\n    182     raise\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:179, in DataLoader.create_batch(self, b)\n    178 def create_batch(self, b): \n--&gt; 179     try: return (fa_collate,fa_convert)[self.prebatched](b)\n    180     except Exception as e: \n    181         if not self.prebatched: collate_error(e,b)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:52, in fa_collate(t)\n     49 \"A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\"\n     50 b = t[0]\n     51 return (default_collate(t) if isinstance(b, _collate_types)\n---&gt; 52         else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     53         else default_collate(t))\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:52, in &lt;listcomp&gt;(.0)\n     49 \"A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\"\n     50 b = t[0]\n     51 return (default_collate(t) if isinstance(b, _collate_types)\n---&gt; 52         else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     53         else default_collate(t))\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/data/load.py:51, in fa_collate(t)\n     49 \"A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s\"\n     50 b = t[0]\n---&gt; 51 return (default_collate(t) if isinstance(b, _collate_types)\n     52         else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence)\n     53         else default_collate(t))\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:317, in default_collate(batch)\n    256 def default_collate(batch):\n    257     r\"\"\"\n    258     Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\n    259 \n   (...)\n    315         &gt;&gt;&gt; default_collate(batch)  # Handle `CustomType` automatically\n    316     \"\"\"\n--&gt; 317     return collate(batch, collate_fn_map=default_collate_fn_map)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:146, in collate(batch, collate_fn_map)\n    144     for collate_type in collate_fn_map:\n    145         if isinstance(elem, collate_type):\n--&gt; 146             return collate_fn_map[collate_type](batch, collate_fn_map=collate_fn_map)\n    148 if isinstance(elem, collections.abc.Mapping):\n    149     try:\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:214, in collate_tensor_fn(batch, collate_fn_map)\n    212     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n    213     out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n--&gt; 214 return torch.stack(batch, 0, out=out)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/fastai/torch_core.py:382, in TensorBase.__torch_function__(cls, func, types, args, kwargs)\n    380 if cls.debug and func.__name__ not in ('__str__','__repr__'): print(func, types, args, kwargs)\n    381 if _torch_handled(args, cls._opt, func): types = (torch.Tensor,)\n--&gt; 382 res = super().__torch_function__(func, types, args, ifnone(kwargs, {}))\n    383 dict_objs = _find_args(args) if args else _find_args(list(kwargs.values()))\n    384 if issubclass(type(res),TensorBase) and dict_objs: res.set_meta(dict_objs[0],as_copy=True)\n\nFile ~/miniconda3/envs/dev/lib/python3.10/site-packages/torch/_tensor.py:1437, in Tensor.__torch_function__(cls, func, types, args, kwargs)\n   1434     return NotImplemented\n   1436 with _C.DisableTorchFunctionSubclass():\n-&gt; 1437     ret = func(*args, **kwargs)\n   1438     if func in get_default_nowrap_functions():\n   1439         return ret\n\nRuntimeError: Error when trying to collate the data into batches with fa_collate, at least two tensors in the batch are not the same size.\n\nMismatch found on axis 0 of the batch and is of type `TensorImage`:\n    Item at index 0 has shape: torch.Size([3, 500, 375])\n    Item at index 1 has shape: torch.Size([3, 333, 500])\n\nPlease include a transform in `after_item` that ensures all data of type TensorImage is the same size\n\n\n\nOpps! We have an error. If we follow the trace that summary gives us, we can see that we are able to build up a single example of our data easily (which means we can bulid up a Datasets object). However, we break when we attempt to collate our data into batches.\nWhy is that? Well, our summary tels us:\nMismatch found on axis 0 of the batch and is of type `TensorImage`:\n    Item at index 0 has shape: torch.Size([3, 500, 375])\n    Item at index 1 has shape: torch.Size([3, 333, 500])\nOur images are of different sizes! DataBlock gives us a method to change this. We can pass in a transform to item_tfms to apply a transform to our independent variables. This should fix the issue.\n\ndblock = DataBlock(\n    blocks = (ImageBlock, CategoryBlock),\n    get_items = get_image_files,\n    get_y = get_y,\n    splitter = RandomSplitter(valid_pct=0.2, seed=42),\n    item_tfms = Resize(224), # resize each image to 224x224\n)\n\ndblock.summary(path/\"images\")\n\nSetting-up type transforms pipelines\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: PILBase.create\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying PILBase.create gives\n      PILImage mode=RGB size=375x500\n  Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\n    starting from\n      /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images/saint_bernard_60.jpg\n    applying get_y gives\n      dog\n    applying Categorize -- {'vocab': None, 'sort': True, 'add_na': False} gives\n      TensorCategory(1)\n\nFinal sample: (PILImage mode=RGB size=375x500, TensorCategory(1))\n\n\nCollecting items from /Users/giannicrivello/.fastai/data/oxford-iiit-pet/images\nFound 7390 items\n2 datasets of sizes 5912,1478\nSetting up Pipeline: PILBase.create\nSetting up Pipeline: get_y -&gt; Categorize -- {'vocab': None, 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} -&gt; ToTensor\n    starting from\n      (PILImage mode=RGB size=375x500, TensorCategory(1))\n    applying Resize -- {'size': (224, 224), 'method': 'crop', 'pad_mode': 'reflection', 'resamples': (&lt;Resampling.BILINEAR: 2&gt;, &lt;Resampling.NEAREST: 0&gt;), 'p': 1.0} gives\n      (PILImage mode=RGB size=224x224, TensorCategory(1))\n    applying ToTensor gives\n      (TensorImage of size 3x224x224, TensorCategory(1))\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nApplying batch_tfms to the batch built\n  Pipeline: IntToFloatTensor -- {'div': 255.0, 'div_mask': 1}\n    starting from\n      (TensorImage of size 4x3x224x224, TensorCategory([1, 0, 1, 1], device='mps:0'))\n    applying IntToFloatTensor -- {'div': 255.0, 'div_mask': 1} gives\n      (TensorImage of size 4x3x224x224, TensorCategory([1, 0, 1, 1], device='mps:0'))\n\n\nAwesome. Now we are ready to stamp out our Dataloaders object and train a model. But first, let’s take a quick digresstion to talk about Dataloaders.",
    "crumbs": [
      "DataBlock Walkthrough"
    ]
  },
  {
    "objectID": "lazy evaluation framework.html",
    "href": "lazy evaluation framework.html",
    "title": "A Framework for Lazy Evaluation of Language Models",
    "section": "",
    "text": "The motivation behind implementing lazy evaluation in language models stems from several key observations:\n\nPedagogical Effectiveness: Traditional tutoring methods often involve guiding students through problems step-by-step, allowing them to think critically and make connections on their own. AI tutors should aim to replicate this approach rather than simply providing answers.\nResource Efficiency: Generating complete solutions upfront is computationally expensive, especially for complex problems. A lazy evaluation approach can significantly reduce resource usage by generating only the necessary information on demand.\nAdaptability: Students have varying levels of understanding and may require different amounts of guidance. A lazy evaluation system can essentially adapt to each student’s needs, providing more or less detail as required.\nEngagement: By revealing information gradually, we can maintain student engagement and encourage active participation in the problem-solving process.\nReal-world Problem Solving: In many real-world scenarios, solutions are not immediately apparent and must be approached incrementally. Training students to think in this way prepares them for challenges beyond the classroom.",
    "crumbs": [
      "A Framework for Lazy Evaluation of Language Models"
    ]
  },
  {
    "objectID": "lazy evaluation framework.html#motivation",
    "href": "lazy evaluation framework.html#motivation",
    "title": "A Framework for Lazy Evaluation of Language Models",
    "section": "",
    "text": "The motivation behind implementing lazy evaluation in language models stems from several key observations:\n\nPedagogical Effectiveness: Traditional tutoring methods often involve guiding students through problems step-by-step, allowing them to think critically and make connections on their own. AI tutors should aim to replicate this approach rather than simply providing answers.\nResource Efficiency: Generating complete solutions upfront is computationally expensive, especially for complex problems. A lazy evaluation approach can significantly reduce resource usage by generating only the necessary information on demand.\nAdaptability: Students have varying levels of understanding and may require different amounts of guidance. A lazy evaluation system can essentially adapt to each student’s needs, providing more or less detail as required.\nEngagement: By revealing information gradually, we can maintain student engagement and encourage active participation in the problem-solving process.\nReal-world Problem Solving: In many real-world scenarios, solutions are not immediately apparent and must be approached incrementally. Training students to think in this way prepares them for challenges beyond the classroom.",
    "crumbs": [
      "A Framework for Lazy Evaluation of Language Models"
    ]
  },
  {
    "objectID": "lazy evaluation framework.html#a-note-on-lazy-evaluation",
    "href": "lazy evaluation framework.html#a-note-on-lazy-evaluation",
    "title": "A Framework for Lazy Evaluation of Language Models",
    "section": "A note on lazy evaluation",
    "text": "A note on lazy evaluation\nThe concept of lazy evaluation is well-established in programming languages, where it refers to the practice of delaying the evaluation of an expression until its value is actually needed. By applying this principle to language models in an educational context, we can create AI tutors that guide students through problems more naturally and effectively.\nWe first start by creating a Lazy State class, which will be used to keep track of the problem, steps, and the current step\n\nsource\n\nLazyState\n\n LazyState (problem:str, steps:List[str]=&lt;factory&gt;, current_step:int=0)\n\n\nstate = LazyState(problem=\"What is the result of f(x) = 3x + 2 when x = 5?\")\nstate.add_step(\"First, we need substitute x in the function with 5\")\nstate\n\nLazyState(problem='What is the result of f(x) = 3x + 2 when x = 5?', steps=['What is the result of f(x) = 3x + 2 when x = 5?', 'First, we need substitute x in the function with 5'], current_step=1)\n\n\n\nstate.get_context()\n\n\"Problem: What is the result of f(x) = 3x + 2 when x = 5? \\n Steps so far: ['What is the result of f(x) = 3x + 2 when x = 5?', 'First, we need substitute x in the function with 5']\"\n\n\nWith this simple state manager, we have a way to track each step of the problem-solving process and get the current context to be used for a call to a language model.\nNow, let’s set up a class LazyEvaluationClient that will do the heavy lifting of managing the state and calling the language model.\n\nsource\n\n\nLLM\n\n LLM (client:anthropic.lib.vertex._client.AnthropicVertex, model:str)\n\n\nsource\n\n\nLazyEvaluationClient\n\n LazyEvaluationClient (llm:__main__.LLM, max_tokens:int=100,\n                       state:Optional[__main__.LazyState]=None)\n\nThe Lazy Evaluation Client\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nllm\nLLM\n\nthe language model to use, see LLM class\n\n\nmax_tokens\nint\n100\nthe maximum number of tokens to generate\n\n\nstate\nOptional\nNone\n\n\n\n\nNow that we have this basic machinery in place, let’s see the system in action with our previous problem\nAs a recap, here is our problem:\n\nfor step in state.steps:\n    print(step)\n\nWhat is the result of f(x) = 3x + 2 when x = 5?\nThe next step in solving this problem is to substitute the given value of x into the function f(x). Let's do that:\n\nStep: Substitute x = 5 into the function f(x) = 3x + 2\n\nf(5) = 3(5) + 2\n\nThis step replaces all instances of x in the function with the given value of 5. This sets us up to perform the calculations in the next step.\nThe next step in solving this problem is to perform the multiplication inside the parentheses:\n\nStep: Calculate 3(5)\n\nf(5) = 3(5) + 2\nf(5) = 15 + 2\n\nIn this step, we multiply 3 by 5, which gives us 15. This simplifies our equation, leaving us with a simple addition to complete in the next step.\nThe next step in solving this problem is to perform the final addition:\n\nStep: Calculate 15 + 2\n\nf(5) = 15 + 2\nf(5) = 17\n\nIn this step, we add 15 and 2, which gives us the final result of 17. This completes the calculation of f(5).\nThe next step in this problem-solving process is to state the final answer:\n\nStep: State the final result\n\nThe result of f(x) = 3x + 2 when x = 5 is 17.\n\nThis step concludes the problem by clearly stating the final answer we calculated in the previous steps.\nPROBLEM DONE\n\n\nNow, let’s set up our client\n\nclient = AnthropicVertex(project_id=project_id, region=location)\nllm = LLM(client=client, model=model)\nlazy_lm = LazyEvaluationClient(llm=llm, state=state)\n\nWe can see what the current step our model is on:\n\nlazy_lm.get_current_step()\n\n'PROBLEM DONE'\n\n\nNow, let’s have our model generate the next step\n\nlazy_lm.get_next_step()\n\n'PROBLEM DONE'\n\n\nFinally, let’s continue calling the model until we reach the end of our problem\n\nwhile True:\n    next_step = lazy_lm.get_next_step()\n    if next_step == \"PROBLEM DONE\":\n        print(\"Problem solved!\")\n        break\n    print(next_step)\n\nThe next step in solving this problem would be to evaluate the expression after substitution. So, the next step is:\n\nEvaluate f(5) = 3(5) + 2:\nf(5) = 15 + 2\nThe next step in solving this problem would be to perform the final calculation. So, the next step is:\n\nCalculate the final result:\nf(5) = 15 + 2 = 17\nThe next step in this problem-solving process would be to state the final answer. So, the next step is:\n\nTherefore, the result of f(x) = 3x + 2 when x = 5 is 17.\nProblem solved!\n\n\n\nsource\n\n\nLazyEvaluationClient.ask_question\n\n LazyEvaluationClient.ask_question (question:str)\n\n*Allows the user to ask a question about the current step without affecting the model’s ability to generate the next step.\nArgs: question (str): The question the user wants to ask about the current step.\nReturns: str: The model’s response to the question.*\nWe now have a way to take the current reasoning step and query it without having the model advance to the next step in the problem-solving process.\n\nstate.refresh()\nstate\n\nLazyState(problem='What is the result of f(x) = 3x + 2 when x = 5?', steps=['What is the result of f(x) = 3x + 2 when x = 5?'], current_step=0)\n\n\n\nlazy_lm = LazyEvaluationClient(llm=llm, state=state)\nlazy_lm.get_current_step()\n\n'What is the result of f(x) = 3x + 2 when x = 5?'\n\n\n\nlazy_lm.get_next_step()\n\n'The next step in solving this problem is to substitute the given value of x into the function.\\n\\nStep: Substitute x = 5 into the function f(x) = 3x + 2'\n\n\n\nlazy_lm.ask_question(\"what is substitution?\")\n\n\"Substitution is a mathematical technique where we replace a variable in an equation or expression with a specific value or another expression. It's a fundamental concept in algebra and is used to solve equations, evaluate functions, and simplify expressions.\\n\\nIn the context of functions, substitution involves replacing the variable (usually x) with a given value to calculate the function's output for that specific input.\\n\\nFor example (not related to the current problem):\\nIf we have a function g(x)\"\n\n\n\nlazy_lm.ask_question(\"give me an example\")\n\n\"Certainly! I'll provide an example of substitution that's not related to the current problem.\\n\\nLet's consider a different function: h(x) = x² - 4x + 7\\n\\nIf we want to find the value of h(3), we would substitute x with 3:\\n\\nh(3) = 3² - 4(3) + 7\\n\\nNow we can calculate:\\nh(3) = 9 - 12\"\n\n\n\nlazy_lm.get_next_step()\n\n'The next step is to perform the calculation using the substituted value:\\n\\nStep: Calculate f(5) = 3(5) + 2'\n\n\nNow let’s put this all together in a simple loop:\n\nwhile True:\n    user_input = input(\"Enter a question or command: \")\n    if user_input in [\"next\", \"n\"]:\n        print(\"-------------------------------------------------\")\n        print(\"User asked for next step\")\n        next_step = lazy_lm.get_next_step()\n        if next_step == \"PROBLEM DONE\":\n            print(\"Problem solved!\")\n            break\n        print(\"-------------------------------------------------\")\n        print(f\"Next step: {next_step}\")\n    elif user_input in [\"question\", \"q\"]:\n        user_question = input(\"Enter your question: \")\n        r = lazy_lm.ask_question(user_question)\n        print(\"-------------------------------------------------\")\n        print(f\"User Question: {user_question}\")\n        print(\"-------------------------------------------------\")\n        print(f\"Model Answer: {r}\")\n\n-------------------------------------------------\nUser asked for next step\n-------------------------------------------------\nNext step: The next step in solving this problem is to substitute the given value of x into the function. Here's the step:\n\nSubstitute x = 5 into the function f(x) = 3x + 2\n\nThis step sets up the equation for us to solve in the following steps.\n-------------------------------------------------\nUser asked for next step\n-------------------------------------------------\nNext step: The next step in solving this problem is to perform the substitution and write out the resulting equation. Here's the step:\n\nReplace x with 5 in the equation:\nf(5) = 3(5) + 2\n\nThis step shows us the function with the specific value of x we're working with, preparing us for the final calculations.\n-------------------------------------------------\nUser Question: what is substitution?\n-------------------------------------------------\nModel Answer: Substitution is a mathematical technique where you replace a variable in an equation or expression with a specific value or another expression. It's a fundamental concept in algebra and is used to solve equations, simplify expressions, or evaluate functions.\n\nIn the context of functions, substitution involves replacing the variable (often x) with a given value to determine the function's output for that specific input.\n\nFor example (not related to the current problem):\nIf we have a function g(x)\n-------------------------------------------------\nUser Question: can you provide an example?\n-------------------------------------------------\nModel Answer: Certainly! I'd be happy to provide an example of substituting a value into a function without using the current problem. Here's a different example:\n\nLet's say we have a function g(y) = 2y² - 4y + 1, and we want to find the value when y = 3.\n\nThe step of substituting the value would look like this:\n\ng(3) = 2(3)² - 4(3)\n-------------------------------------------------\nUser Question: can you finish the example?\n-------------------------------------------------\nModel Answer: Certainly! I'd be happy to provide an example of performing a substitution and writing out the resulting equation, but using a different function than the one in the current context.\n\nLet's consider a different function: g(x) = 2x² - 4x + 1\n\nIf we want to find g(3), we would substitute x = 3 into the function. Here's how that step would look:\n\nReplace x with 3 in the equation\n-------------------------------------------------\nUser asked for next step\n-------------------------------------------------\nNext step: The next step in solving this problem is to perform the multiplication inside the parentheses. Here's the step:\n\nCalculate 3(5):\nf(5) = 3(5) + 2\nf(5) = 15 + 2\n\nThis step simplifies the expression by carrying out the multiplication, bringing us closer to the final result.\n-------------------------------------------------\nUser Question: what is the 3(15) syntax mean?\n-------------------------------------------------\nModel Answer: The syntax 3(15) in the current context is actually incorrect. I apologize for the confusion. Let me clarify:\n\nIn the current step, we have:\nf(5) = 3(5) + 2\nf(5) = 15 + 2\n\nHere, 3(5) means 3 multiplied by 5. The parentheses are used to indicate multiplication. It's equivalent to writing 3 ×\n-------------------------------------------------\nUser asked for next step\n-------------------------------------------------\nNext step: The next step in solving this problem is to perform the final addition. Here's the step:\n\nCalculate 15 + 2:\nf(5) = 15 + 2\nf(5) = 17\n\nThis step completes the calculation, giving us the final result of the function when x = 5.\n-------------------------------------------------\nUser asked for next step\n-------------------------------------------------\nNext step: The next step in this problem-solving process is to state the final answer clearly. Here's the step:\n\nTherefore, the result of f(x) = 3x + 2 when x = 5 is 17.\n\nThis step concludes the problem by explicitly stating the answer to the original question.\n-------------------------------------------------\nUser asked for next step\nProblem solved!",
    "crumbs": [
      "A Framework for Lazy Evaluation of Language Models"
    ]
  },
  {
    "objectID": "lazy evaluation framework.html#the-lazy-evaluation-flow",
    "href": "lazy evaluation framework.html#the-lazy-evaluation-flow",
    "title": "A Framework for Lazy Evaluation of Language Models",
    "section": "The Lazy Evaluation Flow",
    "text": "The Lazy Evaluation Flow\nThis simple framework effectivly shows how we can wrape a language model capable of step-by-step reasoning to create a lazy evaluator.\nThis approach follows these system design steps:\n\nProblem Initialization: A state manager is initalized with a problem\nPrompting Strategy: Prompt the language model to generate the next step given the context in the state manager.\nState Update: State Manager records the newly generated step and updates.\nUser Interaction: User interaction is held within a different state manager question_history which does not affect the overall state of the current problem.\nAdaptive Response: Based on the current input, the Lazy Evaluator decides to either 1) Generate the next step or 2) Provide a response to the user’s question given the current state of the problem and the question history.\n\nTo tie everything together, let’s now add a patch to the AnthropicVertex client to allow users of the framework to have a single entry point into what we can call “lazy mode”.\n\nsource\n\nAnthropicVertex.lazy\n\n AnthropicVertex.lazy (problem:str)\n\nInitialize a lazy evaluation client with a problem\n\nclient = AnthropicVertex(project_id=project_id, region=location)\nlazy_lm = client.lazy(\"What is the result of f(x) = 3x + 2 when x = 5?\")\n\n\nlazy_lm.get_current_step()\n\n'What is the result of f(x) = 3x + 2 when x = 5?'\n\n\n\nlazy_lm.get_next_step()\n\n\"To solve this problem, we need to substitute x with 5 in the given function. Let's do that in the next step:\\n\\nReplace x with 5 in the function f(x) = 3x + 2\\n\\nThis step sets up the equation for us to solve in the following steps.\"",
    "crumbs": [
      "A Framework for Lazy Evaluation of Language Models"
    ]
  }
]